{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8adf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import dateparser\n",
    "from datetime import datetime\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c095bd",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe5cb7",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c87bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateRecord():\n",
    "    def __init__(self, educations, experiences, seniority):\n",
    "        self.educations = educations\n",
    "        self.experiences = experiences\n",
    "        self.seniority = seniority\n",
    "\n",
    "class EducationRecord():\n",
    "    def __init__(self, school, description, degree, time):\n",
    "        self.school = school\n",
    "        self.description = description\n",
    "        self.degree = degree\n",
    "        self.time = time\n",
    "\n",
    "class ExperienceRecord():\n",
    "    def __init__(self, work, description, title, time, skills):\n",
    "        self.employer = work\n",
    "        self.description = description\n",
    "        self.title = title\n",
    "        self.skills = skills\n",
    "        self.time = time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034f2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepath = \"./data/seniority.train\"\n",
    "test_filepath = \"./data/seniority.test\"\n",
    "\n",
    "def load_data(filepath):\n",
    "    candidates = []\n",
    "    with open(filepath, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line.strip())\n",
    "            educations = [EducationRecord(e['school'], e['description'], e['degree'], e['time']) for e in record['education']]\n",
    "            experiences =[ExperienceRecord(e['work'], e['description'], e['title'], e['time'], e['skills']) for e in record['experience']]\n",
    "            candidate = CandidateRecord(educations, experiences, record['seniority_level'])\n",
    "            candidates.append(candidate)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "train_candidates = load_data(train_filepath)\n",
    "test_candidates = load_data(test_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563d4a9",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80ae502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29969, 29999)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_candidates), len(test_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec0913fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen = [c.seniority for c in train_candidates]\n",
    "test_sen = [c.seniority for c in test_candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a5d7e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'Entry': 9927,\n",
       "          'Senior': 3564,\n",
       "          'Mid-Level': 8947,\n",
       "          'Intern': 928,\n",
       "          'Manager': 3892,\n",
       "          'Director': 1768,\n",
       "          None: 294,\n",
       "          'Vice President': 447,\n",
       "          'CXO': 202}),\n",
       " Counter({'Mid-Level': 9090,\n",
       "          'Manager': 4073,\n",
       "          'Senior': 4196,\n",
       "          'Director': 2135,\n",
       "          'Entry': 8362,\n",
       "          'Vice President': 674,\n",
       "          None: 272,\n",
       "          'CXO': 303,\n",
       "          'Intern': 894}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_sen), Counter(test_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2145eeb",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e8171",
   "metadata": {},
   "source": [
    "#### Time data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae24800",
   "metadata": {},
   "source": [
    "Time data seems to have various formats and arbitrary values. We will have to clean these to extract duration of work experience or figure out if the candidate has completed their degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b98a8d",
   "metadata": {},
   "source": [
    "The following code was written iteratively:\n",
    "- First I eye-balled the time data for various samples\n",
    "- Then I looked at text-only time data\n",
    "- Then I looked at unknown time data (empty or \"n/a\" or \"unknown\")\n",
    "- Then I added education/experience count to check if total count makes sense\n",
    "    - This revealed that for some education/experience records, we have a single time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b0fc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'', 'notKnown'}, 17054, 22866, 72403, 48242)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_times = set()\n",
    "textual_times = set()\n",
    "textual_times_count = 0\n",
    "empty_times_count = 0\n",
    "total_times_count = 0\n",
    "education_count = 0\n",
    "for c in train_candidates:\n",
    "    for e in c.educations:\n",
    "        education_count += 1\n",
    "        if e.time == []:\n",
    "            empty_times_count += 2\n",
    "        for time in e.time:\n",
    "            total_times_count += 1\n",
    "            try:\n",
    "                if bool(re.match(r'^\\D*$', time)):\n",
    "                    textual_times.add(time)\n",
    "                    textual_times_count += 1\n",
    "            except Exception as e:\n",
    "                error_times.add(time)\n",
    "\n",
    "textual_times, textual_times_count, empty_times_count, total_times_count, education_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92252627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'',\n",
       "  'Aujourd’hui',\n",
       "  'Current',\n",
       "  'N/A',\n",
       "  'Present',\n",
       "  'Presente',\n",
       "  'Till Date',\n",
       "  'current',\n",
       "  'notKnown',\n",
       "  'present'},\n",
       " 989,\n",
       " 27095,\n",
       " 28,\n",
       " 293478,\n",
       " 152830)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_times = set()\n",
    "textual_times = set()\n",
    "textual_times_count = 0\n",
    "empty_times_count = 0\n",
    "total_times_count = 0\n",
    "unknown_times_count = 0\n",
    "experience_count = 0\n",
    "for c in train_candidates:\n",
    "    for e in c.experiences:\n",
    "        experience_count += 1\n",
    "        if e.time == []:\n",
    "            empty_times_count += 2\n",
    "            unknown_times_count += 2\n",
    "        for time in e.time:\n",
    "            total_times_count += 1\n",
    "            try:\n",
    "                if time in ['', 'N/A', 'notKnown']:\n",
    "                    unknown_times_count += 1\n",
    "                if bool(re.match(r'^\\D*$', time)):\n",
    "                    textual_times.add(time)\n",
    "                    textual_times_count += 1\n",
    "            except Exception as e:\n",
    "                error_times.add(time)\n",
    "\n",
    "textual_times, unknown_times_count, textual_times_count, empty_times_count, total_times_count, experience_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df3409",
   "metadata": {},
   "source": [
    "So, for ~12% (0.5 * 23k/95k) of the education data and for ~0.16% (0.5 * 1k/300k) of the experience data, we do not have a valid time.\n",
    "\n",
    "Assumptions:\n",
    "1. I think we can ignore the time of graduation altogether since we do not have plan to extract any feature based on it. If we do want to check if, say, the person has completed their degree, then we would need a valid end time.\n",
    "\n",
    "2. We can treat the 0.16% missing times for experience data as missing data. Can choose to ignore it considering its low volume.\n",
    "\n",
    "3. I ran dateparser.parse() on all the above times and it ran successfully. Thus, I am assuming we either have valid dates or \"text-only\" dates that are shown above. If there is any date like \"2022abc\", then we count it as just another invalid date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2761c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dateparser.parse(\"2022abc\")\n",
    "a == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b89d3",
   "metadata": {},
   "source": [
    "### College Degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f086f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5042fd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48242,\n",
       " ['Master of Business Administration - MBA',\n",
       "  'Bachelors of Science in Business Administration with a Concentration in Marketing',\n",
       "  '',\n",
       "  'Master of Science in Finance',\n",
       "  'B.A in Applied Mathematics',\n",
       "  'GED',\n",
       "  'M.S.',\n",
       "  'Bachelor of Science',\n",
       "  'Bachelor of Arts',\n",
       "  ''])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degrees = []\n",
    "for c in train_candidates:\n",
    "    for e in c.educations:\n",
    "        degrees.append(e.degree)\n",
    "    \n",
    "len(degrees), degrees[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4449c6be",
   "metadata": {},
   "source": [
    "Need to remove punctuation to get terms like \"be\", \"bsc\", \"msc\" instead of \"b\", \"e\", \"sc\", \"m\"\n",
    "\n",
    "The above list is also cluttered by department names. We can use a standand list of departments at stop words to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74f37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.mydegreeguide.com/types-of-degrees/types-of-bachelor-degree/\n",
    "# The dept list given above is augmented with the individual tokens found in the dept names itself e.g. \"business administration\" => \"business\", \"administration\"\n",
    "depts = [\"architecture\", \"arts\", \"business\", \"business administration\", \"science in business\", \"canon law\", \"computer science\", \"science in computer science\", \"criminal justice\", \"science in criminal justice\", \"divinity\", \"education\", \"science in education\", \"wireless engineering\", \"engineering\", \"science in engineering\", \"science in aerospace engineering\", \"science in agricultural engineering\", \"science in biological systems\", \"science in biosystems and agricultural engineering\", \"science in biological engineering\", \"biomedical engineering\", \"science in biomedical engineering\", \"science in chemical engineering\", \"science in chemical and biomolecular engineering\", \"science in chemical and materials engineering\", \"civil engineering\", \"science in civil engineering\", \"science in civil and infrastructure engineering\", \"computer engineering\", \"science in computer engineering\", \"science in computer science and engineering\", \"science in electrical and computer engineering\", \"electrical engineering\", \"science in electrical engineering\", \"science in engineering management\", \"science in environmental engineering\", \"fiber engineering\", \"science in industrial engineering\", \"science in manufacturing engineering\", \"science in manufacturing systems engineering\", \"science in materials science and engineering\", \"science in materials engineering\", \"mechanical engineering\", \"science in mechanical engineering\", \"science in metallurgical engineering\", \"science in mining engineering\", \"science in systems\", \"software engineering\", \"science in software engineering\", \"systems engineering\", \"science in systems engineering\", \"engineering technology\", \"science in engineering technology\", \"science in civil engineering technology\", \"science in computer engineering technology\", \"science in construction engineering technology\", \"science in drafting design technology\", \"science in electrical/electronics technology\", \"science in electrical engineering technology\", \"science in electro-mechanical engineering technology\", \"science in mechanical engineering technology\", \"fine arts\", \"forestry\", \"science in forest research\", \"hebrew letters\", \"journalism\", \"laws\", \"liberal studies\", \"literature\", \"marine science\", \"music\", \"nursing\", \"science in nursing\", \"pharmacy\", \"philosophy\", \"religious education\", \"science\", \"science in chemistry\", \"technology\", 'administration', 'aerospace', 'agricultural', 'and', 'biological', 'biomedical', 'biomolecular', 'biosystems', 'canon', 'chemical', 'chemistry', 'civil', 'computer', 'construction', 'criminal', 'design', 'drafting', 'electrical', 'electro', 'electronics', 'environmental', 'fiber', 'fine', 'forest', 'hebrew', 'in', 'industrial', 'infrastructure', 'justice', 'law', 'letters', 'liberal', 'management', 'manufacturing', 'marine', 'materials', 'mechanical', 'metallurgical', 'mining', 'religious', 'research', 'software', 'studies', 'systems', 'wireless']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ecf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [s.translate(str.maketrans('', '', string.punctuation)) for s in degrees]\n",
    "\n",
    "degree_vectorizer = CountVectorizer(stop_words=depts + [\"of\"], ngram_range=(1, 2), max_features=50)\n",
    "cv_fit = degree_vectorizer.fit_transform(degrees)\n",
    "popular_degrees = degree_vectorizer.get_feature_names_out()\n",
    "popular_degree_freq = cv_fit.toarray().sum(axis=0)\n",
    "\n",
    "sorted(zip(popular_degree_freq, popular_degrees), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601d2d8",
   "metadata": {},
   "source": [
    "\n",
    "This looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c668f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(degree_vectorizer.transform([\"bs\", \"ms\"])).A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9655e",
   "metadata": {},
   "source": [
    "\n",
    "For titles, we don't need to remove any punctuation or other terms. We can simply count the n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48efa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for c in train_candidates:\n",
    "    for e in c.experiences:\n",
    "        titles.append(e.title)\n",
    "    \n",
    "len(titles), titles[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 3), max_features=100)\n",
    "cv_fit = title_vectorizer.fit_transform(titles)\n",
    "popular_titles = title_vectorizer.get_feature_names_out()\n",
    "popular_title_freq = cv_fit.toarray().sum(axis=0)\n",
    "\n",
    "sorted(zip(popular_title_freq, popular_titles), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ee0f0",
   "metadata": {},
   "source": [
    "This list looks decent. Most of the terms give an indication of the seniority of the corresponding role. I tried `max_features = 200` and saw not so informative terms at the end, so switched to 100 and we can see that the last few terms also have good indicative information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6262f9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(title_vectorizer.transform([\"software engineer\", \"machine learning engineer\"])).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ad855",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = []\n",
    "for c in train_candidates:\n",
    "    for e in c.experiences:\n",
    "        if e.skills:\n",
    "            skills += e.skills\n",
    "    \n",
    "len(skills), skills[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52618405",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_vectorizer_unigram = CountVectorizer(max_features=200)\n",
    "cv_fit = skills_vectorizer_unigram.fit_transform(skills)\n",
    "popular_skills = skills_vectorizer_unigram.get_feature_names_out()\n",
    "popular_skill_freq = cv_fit.toarray().sum(axis=0)\n",
    "\n",
    "sorted(zip(popular_skill_freq, popular_skills), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399a833",
   "metadata": {},
   "source": [
    "If we looked at skills n-grams in range (1, 3), we notice that they are dominated by unigrams. Thus, I have decided to take 200 unigrams and 100 bi/tri-grams (most of which are bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_vectorizer_bitri = CountVectorizer(max_features=100, ngram_range=(2, 3))\n",
    "cv_fit = skills_vectorizer_bitri.fit_transform(skills)\n",
    "popular_skills = skills_vectorizer_bitri.get_feature_names_out()\n",
    "popular_skill_freq = cv_fit.toarray().sum(axis=0)\n",
    "\n",
    "sorted(zip(popular_skill_freq, popular_skills), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b392a275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(skills_vectorizer_unigram.transform([\"technology\", \"engineer\", \"communication\"])).A[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7206237f",
   "metadata": {},
   "source": [
    "We can use the tokenizer from Spacy if needed. Note we are using the English tokenizer here but it seems to do a decent job on French sentences too.\n",
    "\n",
    "I tried using this tokenizer instead of the default tokenizer in CountVectorizer but the combination was not playing well around punctuations. And the default tokenizer does a decent job anyway, so I decided to keep it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e8dbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "doc = tokenizer(\"inventory.\\n\\u25cf Unloaded freights/pallets from trailers.\\n\\u25cf Stocked and organized shelves according to procedures and policies.\\n\\u25cf Staged promotional\")\n",
    "frenchdoc = tokenizer(\"\\u2022 V\\u00e9rifier les ingr\\u00e9dients d'adh\\u00e9sif pr\\u00e9par\\u00e9s par le journalier (deuxi\\u00e8me compte des ingr\\u00e9dients).\\n\\u2022 Ins\\u00e9rer les ingr\\u00e9dients dans le m\\u00e9langeur selon la proc\\u00e9dure \\u00e9tablie, afin d'obtenir un produit de premi\\u00e8re qualit\\u00e9 et ainsi rencontrer les normes de tests \\u00e9tablies.\\n\\u2022 Proc\\u00e9der au set-up de la ligne d'extrusion, c'est-\\u00e0-dire couteau, rouleau, \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab8f2980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(inventory.\n",
       " ● Unloaded freights/pallets from trailers.\n",
       " ● Stocked and organized shelves according to procedures and policies.\n",
       " ● Staged promotional,\n",
       " 27,\n",
       " inventory,\n",
       " spacy.tokens.token.Token)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc, len(doc), doc[0], type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fec9f7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(• Vérifier les ingrédients d'adhésif préparés par le journalier (deuxième compte des ingrédients).\n",
       " • Insérer les ingrédients dans le mélangeur selon la procédure établie, afin d'obtenir un produit de première qualité et ainsi rencontrer les normes de tests établies.\n",
       " • Procéder au set-up de la ligne d'extrusion, c'est-à-dire couteau, rouleau, ,\n",
       " 66,\n",
       " Vérifier)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchdoc, len(frenchdoc), frenchdoc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40e72f",
   "metadata": {},
   "source": [
    "### Test out DistilBert's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1a6b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_to_embed = \"\"\"\n",
    "This is a really long piece of text... The AI / Machine Learning team is building the industry's leading Machine Learning models for use cases at scale. This team is also responsible for pushing the boundaries of applied Machine Learning on a challenging and diverse dataset.  \n",
    "What you will do (learn to do) \n",
    "You will own, train, build and deploy cutting edge models across all Eightfold products, end to end- in production \n",
    "Create innovative algorithms for Machine Learning & AI \n",
    "Create industry best practices for Machine Learning in Recruiting and HR \n",
    "Create industry best practices for Machine Learning in Recruiting and HR \n",
    "Create industry best practices for Machine Learning in Recruiting and HR \n",
    "Do it responsibly to provide equal opportunity for everyone by extending our internal model fairness platform \n",
    "Create innovative algorithms for Machine Learning & AI \n",
    "Analyze data to identify actionable insights and optimize deployment results \n",
    "Develop AI-based systems for Natural Language Processing (NLP) \n",
    "Implement best practices for building AI-enabled products \n",
    "Utilize Python programming language to web scrape and develop databases \n",
    "Optimize Machine Learning models for time efficiency, performance, cost, scalability, and accuracy \n",
    "Utilize state-of-the-art techniques such as AutoML and transfer learning \n",
    "Develop tools and processes for automatically extracting information from text using Bert \n",
    "Do it responsibly to provide equal opportunity for everyone by extending our internal model fairness platform \n",
    "Create innovative algorithms for Machine Learning & AI \n",
    "Analyze data to identify actionable insights and optimize deployment results \n",
    "Develop AI-based systems for Natural Language Processing (NLP) \n",
    "Implement best practices for building AI-enabled products \n",
    "Utilize Python programming language to web scrape and develop databases \n",
    "Optimize Machine Learning models for time efficiency, performance, cost, scalability, and accuracy \n",
    "Utilize state-of-the-art techniques such as AutoML and transfer learning \n",
    "Develop tools and processes for automatically extracting information from text using Bert \n",
    "Do it responsibly to provide equal opportunity for everyone by extending our internal model fairness platform \n",
    "Create innovative algorithms for Machine Learning & AI \n",
    "Analyze data to identify actionable insights and optimize deployment results \n",
    "Develop AI-based systems for Natural Language Processing (NLP) \n",
    "Implement best practices for building AI-enabled products \n",
    "Utilize Python programming language to web scrape and develop databases \n",
    "Optimize Machine Learning models for time efficiency, performance, cost, scalability, and accuracy \n",
    "Utilize state-of-the-art techniques such as AutoML and transfer learning \n",
    "Develop tools and processes for automatically extracting information from text using Bert \n",
    "What you bring \n",
    "Good foundation in Machine Learning (ML), Deep Learning, and NLP \n",
    "Hands-on experience in applying Natural Language Processing (NLP) solutions is a plus. \n",
    "Ability to work cross functionally & interface with data science experts \n",
    "Familiar with Language models, transformers like BERT, GPT-3, T-5 etc. \n",
    "Strong knowledge of CS fundamental concepts and ML languages ( like Python, C, C++, Java, JavaScript, R, and Scala, etc. ) \n",
    "Ability to innovate, as proven by a track record of software artifacts or academic publications in applied machine learning. \n",
    "Understanding of data and/or ML systems with ability to think across layers of the stack - REST APIs, microservices, data ingestion and processing systems, and distributed systems. \n",
    "Nice To Haves \n",
    "Experience with scientific libraries in Python (numba, pandas) and machine learning tools and frameworks (scikit-learn, tensorflow, torch, etc.). \n",
    "Experience of implementing production machine learning systems, working with large scale datasets is a plus \n",
    "Solid understanding of machine learning theory \n",
    "Metrics-focused and passionate about delivering high quality models. \n",
    "Familiar with Pandas or Python machine learning libraries \n",
    "Familiar with Spark, MLLib, Databricks MLFlow, Apache Airflow and similar related technologies. \n",
    "Familiar with a cloud based environment such as AWS \n",
    "Experience with analyzing large data sets, using Hadoop, Spark or related technologies is a plus. \n",
    "CA Pay Transparency \n",
    "The information below is provided for candidates hired in California location. \n",
    "In California, the standard base pay range for this role is USD $121,000 - $163,000 annually. \n",
    "In addition to a competitive base salary, this position is also eligible for equity awards, benefits and discretionary bonuses. A candidate’s salary is determined by various factors including, but not limited to, experience, skills, and geographic location within the state. \n",
    "The information below is provided for candidates hired in California location. \n",
    "In California, the standard base pay range for this role is USD $121,000 - $163,000 annually. \n",
    "In addition to a competitive base salary, this position is also eligible for equity awards, benefits and discretionary bonuses. A candidate’s salary is determined by various factors including, but not limited to, experience, skills, and geographic location within the state\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93bc8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 5, 5]]\n"
     ]
    }
   ],
   "source": [
    "def break_list_into_chunks(lst, K, pad_item):\n",
    "    # Calculate the number of chunks required\n",
    "    num_chunks = (len(lst) + K - 1) // K\n",
    "\n",
    "    # Initialize an empty list to store the result\n",
    "    result = []\n",
    "\n",
    "    # Break the list into chunks of size K\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * K\n",
    "        end_idx = (i + 1) * K\n",
    "        chunk = lst[start_idx:end_idx]\n",
    "\n",
    "        # Pad the last chunk if its length is less than K\n",
    "        if len(chunk) < K:\n",
    "            chunk.extend([pad_item] * (K - len(chunk)))\n",
    "\n",
    "        result.append(chunk)\n",
    "\n",
    "    return result\n",
    "\n",
    "N = 10\n",
    "K = 3\n",
    "input_list = [i for i in range(N)]\n",
    "padding_item = 5\n",
    "\n",
    "result = break_list_into_chunks(input_list, K, padding_item)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b6bd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.logging import set_verbosity_error\n",
    "# Prevents the warning that shows up whenever our input text length is greater than model's max length\n",
    "set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1018df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.to(0)\n",
    "model = BetterTransformer.transform(model)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446aee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_distilbert_embeddings_averaged(model, tokenizer, text, chunk_size=512):\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    chunks = break_list_into_chunks(input_ids, chunk_size, tokenizer.pad_token_id)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        chunk_embeddings = [model(torch.tensor(chunk).unsqueeze(0).to(0))[0] for chunk in chunks]\n",
    "    \n",
    "    cat_emb = torch.cat(chunk_embeddings)\n",
    "    mean_embedding = torch.mean(cat_emb, dim=[0, 1])\n",
    "    \n",
    "    return mean_embedding\n",
    "\n",
    "chunk_size = 512\n",
    "mean_embedding = get_distilbert_embeddings_averaged(model, tokenizer, text_to_embed, chunk_size)\n",
    "\n",
    "mean_embedding.shape, mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [10]\n",
    "a += (mean_embedding.tolist())\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f618a8a",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93735eb3",
   "metadata": {},
   "source": [
    "Here are some features we can consider for a classification task:\n",
    "1. Years of Work Experience\n",
    "    - Fairly straightforward numerical feature\n",
    "    - We will merge overlapping work experiences for simplicity -- can perhaps do a weighted sum in some fashion\n",
    "2. Education level\n",
    "    - We can have a feature corresponding to an education at each level (certificate, diploma, associate, bachelor, master, phd)\n",
    "    - We can also use a CounterVectorizer -- that should capture the above unigram terms\n",
    "3. Job Title is a strong indicator\n",
    "    - Look for the presence of certain terms, perhaps perform one-hot encoding on the n-grams\n",
    "4. Skills\n",
    "    - Can create feature vectors from skills -- same as above, we can start with a CountVectorizer for the n-grams\n",
    "    - Skills are specific to the industry/field but can be a good indicator of seniority.\n",
    "5. Work and education descriptions\n",
    "    - Can chunk the descriptions and create averaged LLM embeddings of the descriptions\n",
    "6. Company and College ranking based features\n",
    "    - If we have some ranking of various companies, we can perhaps create a feature out of this\n",
    "    - Manager or CEO at Google probably denotes higher seniority than counterparts at other companies\n",
    "\n",
    "\n",
    "*Let's start with the first four features and see how it goes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9648f",
   "metadata": {},
   "source": [
    "### Years of Work Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d136dd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = datetime(2022, 10, 10)\n",
    "b = datetime(2021, 2, 2)\n",
    "a < b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a443b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible values of time: {'', 'Aujourd’hui', 'Current', 'N/A', 'Present', 'Presente', 'Till Date', 'current', 'notKnown', 'present'}\n",
    "unknown_time_values = set(['', 'notKnown', 'N/A', None])\n",
    "today_time_values = set(['Aujourd’hui', 'o momento', 'Current', 'Present', 'Presente', 'Till Date', 'current', 'present'])\n",
    "\n",
    "\n",
    "def merge_overlapping_intervals(array):\n",
    "    array.sort() # sorts by 1st element of the 2-item list\n",
    "    merged = [array[0]]\n",
    "    for current in array:\n",
    "        previous = merged[-1]\n",
    "        if current[0] <= previous[1]:\n",
    "            previous[1] = max(previous[1], current[1])\n",
    "        else:\n",
    "            merged.append(current)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def extract_yoe(candidate: CandidateRecord):\n",
    "    work_time_ranges = []\n",
    "    for e in candidate.experiences:\n",
    "        # If insufficient/invalid times are recorded for a given experience, we ignore them\n",
    "        if len(e.time) < 2 or any([t in unknown_time_values for t in e.time]):\n",
    "            continue\n",
    "        start_time = dateparser.parse(e.time[0]) if e.time[0] not in today_time_values else dateparser.parse('today')\n",
    "        end_time = dateparser.parse(e.time[1]) if e.time[1] not in today_time_values else dateparser.parse('today')\n",
    "        work_time_ranges.append([start_time, end_time])\n",
    "    \n",
    "\n",
    "    if len(work_time_ranges) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        work_time_ranges = merge_overlapping_intervals(work_time_ranges)\n",
    "        yoe_in_months = sum([math.ceil((w[1] - w[0]).days / 30) for w in work_time_ranges]) \n",
    "        return yoe_in_months\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea0eaf",
   "metadata": {},
   "source": [
    "### Creating feature vectors for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72522578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each candidate, we will extract the features and then create a DataFrame from the records\n",
    "# Length of feature vector = 1 (yoe) + 50 (degree) + 100 (title) + 200 (skills_uni) + 100 (skills_bitri) = 451\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_features(c):\n",
    "    c_vector = []\n",
    "\n",
    "    c_vector.append(extract_yoe(c))\n",
    "\n",
    "    degrees = [e.degree for e in c.educations]\n",
    "    degrees = [s.translate(str.maketrans('', '', string.punctuation)) for s in degrees]\n",
    "    c_vector += sum(degree_vectorizer.transform(degrees)).A[0].tolist() if degrees else [0]*50\n",
    "    titles = [e.title for e in c.experiences]\n",
    "    c_vector += sum(title_vectorizer.transform(titles)).A[0].tolist() if titles else [0]*100\n",
    "\n",
    "    c_skills = []\n",
    "    for e in c.experiences:\n",
    "        if e.skills:\n",
    "            c_skills += e.skills\n",
    "    c_vector += sum(skills_vectorizer_unigram.transform(c_skills)).A[0].tolist() if c_skills else [0]*200\n",
    "    c_vector += sum(skills_vectorizer_bitri.transform(c_skills)).A[0].tolist() if c_skills else [0]*100\n",
    "\n",
    "    return c_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9351e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_llm_features(c):\n",
    "    c_vector = []\n",
    "    c_vector.append(extract_yoe(c))\n",
    "\n",
    "    text = \"\"\n",
    "    for e in c.educations:\n",
    "        text +=  (e.school if e.school else \"\") + (e.degree if e.degree else \"\") + (e.description if e.description else \"\")\n",
    "\n",
    "    c_vector += get_distilbert_embeddings_averaged(model, tokenizer, text).tolist()\n",
    "\n",
    "    text = \"\"\n",
    "    for e in c.experiences:\n",
    "        text += (e.employer if e.employer else \"\") + (e.title if e.title else \"\") + (e.description if e.description else \"\") + (' '.join(e.skills) if e.skills else \"\")\n",
    "\n",
    "    c_vector += get_distilbert_embeddings_averaged(model, tokenizer, text).tolist()\n",
    "\n",
    "    return c_vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df0d089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29969/29969 [36:10<00:00, 13.81it/s]  \n",
      " 89%|████████▉ | 26784/29999 [5:59:30<02:54, 18.44it/s]        "
     ]
    }
   ],
   "source": [
    "train_embeddings = []\n",
    "for idx, c in enumerate(tqdm(train_candidates)):\n",
    "    try:\n",
    "        train_embeddings.append(extract_llm_features(c))\n",
    "    except Exception as e:\n",
    "        print(\"Encountered error while parsing train candidate:\", idx)\n",
    "        print(e)\n",
    "\n",
    "test_embeddings = []\n",
    "for idx, c in enumerate(tqdm(test_candidates)):\n",
    "    try:\n",
    "        test_embeddings.append(extract_llm_features(c))\n",
    "    except Exception as e:\n",
    "        print(\"Encountered error while parsing test candidate:\", idx)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31716d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/train_embeddings.pkl\", 'wb') as out:\n",
    "    pickle.dump(train_embeddings, out)\n",
    "with open(\"./data/test_embeddings.pkl\", 'wb') as out:\n",
    "    pickle.dump(test_embeddings, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "for idx, c in enumerate(tqdm(train_candidates)):\n",
    "    try:\n",
    "        train_df.append(extract_features(c))\n",
    "    except Exception as e:\n",
    "        print(\"Encountered error while parsing train candidate:\", idx)\n",
    "        print(e)\n",
    "\n",
    "test_df = []\n",
    "for idx, c in enumerate(tqdm(test_candidates)):\n",
    "    try:\n",
    "        test_df.append(extract_features(c))\n",
    "    except Exception as e:\n",
    "        print(\"Encountered error while parsing test candidate:\", idx)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/train_features.pkl\", 'wb') as out:\n",
    "    pickle.dump(train_df, out)\n",
    "with open(\"./data/test_features.pkl\", 'wb') as out:\n",
    "    pickle.dump(test_df, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86bb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29969, 451, 29999, 451)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(train_df[0]), len(test_df), len(test_df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15e929",
   "metadata": {},
   "source": [
    "### Create train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    None: -1,\n",
    "    \"Intern\": 0,\n",
    "    \"Entry\": 1,\n",
    "    \"Mid-Level\": 2,\n",
    "    \"Senior\": 3,\n",
    "    \"Manager\": 4,\n",
    "    \"Director\": 5,\n",
    "    \"Vice President\": 6,\n",
    "    \"CXO\": 7\n",
    "}\n",
    "\n",
    "y_train_all = [label_map[c.seniority] for c in train_candidates]\n",
    "y_test = [label_map[c.seniority] for c in test_candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c8648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294, 272)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_train_indices = [idx for idx, label in enumerate(y_train_all) if label == -1]\n",
    "bad_test_indices = [idx for idx, label in enumerate(y_test) if label == -1]\n",
    "\n",
    "len(bad_train_indices), len(bad_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d060bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = pd.DataFrame(train_df)\n",
    "X_test = pd.DataFrame(test_df)\n",
    "\n",
    "X_train_all = X_train_all.drop(index=bad_train_indices)\n",
    "X_test = X_test.drop(index=bad_test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a77ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = [label for idx, label in enumerate(y_train_all) if idx not in bad_train_indices]\n",
    "y_test = [label for idx, label in enumerate(y_test) if idx not in bad_test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab764da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29675, 451), (29727, 451), 29675, 29727)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_all.shape, X_test.shape, len(y_train_all), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970cbe5",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07c91e",
   "metadata": {},
   "source": [
    "I did not do comprehensive hyper-parameter tuning with a validation set, but I did try `n_estimators = [100, 200, 300]`, `max_depth = [5, 10, 15]`, and `learning_rate = [0.001, 0.01, 0.02, 0.03]`\n",
    "\n",
    "I found that (300, 15, 0.03) fetched the minimum training error and maximum training accuracy (~97%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93048e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23740, 451), (5935, 451), 23740, 5935)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train.shape, x_val.shape, len(y_train), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4fb9ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.02003\n",
      "[1]\tvalidation_0-mlogloss:1.96600\n",
      "[2]\tvalidation_0-mlogloss:1.91628\n",
      "[3]\tvalidation_0-mlogloss:1.87023\n",
      "[4]\tvalidation_0-mlogloss:1.82737\n",
      "[5]\tvalidation_0-mlogloss:1.78756\n",
      "[6]\tvalidation_0-mlogloss:1.74997\n",
      "[7]\tvalidation_0-mlogloss:1.71464\n",
      "[8]\tvalidation_0-mlogloss:1.68091\n",
      "[9]\tvalidation_0-mlogloss:1.64904\n",
      "[10]\tvalidation_0-mlogloss:1.61875\n",
      "[11]\tvalidation_0-mlogloss:1.59007\n",
      "[12]\tvalidation_0-mlogloss:1.56272\n",
      "[13]\tvalidation_0-mlogloss:1.53685\n",
      "[14]\tvalidation_0-mlogloss:1.51187\n",
      "[15]\tvalidation_0-mlogloss:1.48811\n",
      "[16]\tvalidation_0-mlogloss:1.46515\n",
      "[17]\tvalidation_0-mlogloss:1.44326\n",
      "[18]\tvalidation_0-mlogloss:1.42221\n",
      "[19]\tvalidation_0-mlogloss:1.40211\n",
      "[20]\tvalidation_0-mlogloss:1.38304\n",
      "[21]\tvalidation_0-mlogloss:1.36422\n",
      "[22]\tvalidation_0-mlogloss:1.34615\n",
      "[23]\tvalidation_0-mlogloss:1.32888\n",
      "[24]\tvalidation_0-mlogloss:1.31215\n",
      "[25]\tvalidation_0-mlogloss:1.29607\n",
      "[26]\tvalidation_0-mlogloss:1.28062\n",
      "[27]\tvalidation_0-mlogloss:1.26560\n",
      "[28]\tvalidation_0-mlogloss:1.25117\n",
      "[29]\tvalidation_0-mlogloss:1.23726\n",
      "[30]\tvalidation_0-mlogloss:1.22377\n",
      "[31]\tvalidation_0-mlogloss:1.21090\n",
      "[32]\tvalidation_0-mlogloss:1.19824\n",
      "[33]\tvalidation_0-mlogloss:1.18598\n",
      "[34]\tvalidation_0-mlogloss:1.17395\n",
      "[35]\tvalidation_0-mlogloss:1.16253\n",
      "[36]\tvalidation_0-mlogloss:1.15139\n",
      "[37]\tvalidation_0-mlogloss:1.14069\n",
      "[38]\tvalidation_0-mlogloss:1.13031\n",
      "[39]\tvalidation_0-mlogloss:1.12012\n",
      "[40]\tvalidation_0-mlogloss:1.11050\n",
      "[41]\tvalidation_0-mlogloss:1.10097\n",
      "[42]\tvalidation_0-mlogloss:1.09189\n",
      "[43]\tvalidation_0-mlogloss:1.08295\n",
      "[44]\tvalidation_0-mlogloss:1.07425\n",
      "[45]\tvalidation_0-mlogloss:1.06588\n",
      "[46]\tvalidation_0-mlogloss:1.05771\n",
      "[47]\tvalidation_0-mlogloss:1.04982\n",
      "[48]\tvalidation_0-mlogloss:1.04221\n",
      "[49]\tvalidation_0-mlogloss:1.03484\n",
      "[50]\tvalidation_0-mlogloss:1.02761\n",
      "[51]\tvalidation_0-mlogloss:1.02047\n",
      "[52]\tvalidation_0-mlogloss:1.01366\n",
      "[53]\tvalidation_0-mlogloss:1.00701\n",
      "[54]\tvalidation_0-mlogloss:1.00038\n",
      "[55]\tvalidation_0-mlogloss:0.99404\n",
      "[56]\tvalidation_0-mlogloss:0.98791\n",
      "[57]\tvalidation_0-mlogloss:0.98189\n",
      "[58]\tvalidation_0-mlogloss:0.97599\n",
      "[59]\tvalidation_0-mlogloss:0.97041\n",
      "[60]\tvalidation_0-mlogloss:0.96491\n",
      "[61]\tvalidation_0-mlogloss:0.95960\n",
      "[62]\tvalidation_0-mlogloss:0.95434\n",
      "[63]\tvalidation_0-mlogloss:0.94925\n",
      "[64]\tvalidation_0-mlogloss:0.94431\n",
      "[65]\tvalidation_0-mlogloss:0.93966\n",
      "[66]\tvalidation_0-mlogloss:0.93512\n",
      "[67]\tvalidation_0-mlogloss:0.93051\n",
      "[68]\tvalidation_0-mlogloss:0.92603\n",
      "[69]\tvalidation_0-mlogloss:0.92164\n",
      "[70]\tvalidation_0-mlogloss:0.91745\n",
      "[71]\tvalidation_0-mlogloss:0.91335\n",
      "[72]\tvalidation_0-mlogloss:0.90941\n",
      "[73]\tvalidation_0-mlogloss:0.90546\n",
      "[74]\tvalidation_0-mlogloss:0.90157\n",
      "[75]\tvalidation_0-mlogloss:0.89785\n",
      "[76]\tvalidation_0-mlogloss:0.89420\n",
      "[77]\tvalidation_0-mlogloss:0.89077\n",
      "[78]\tvalidation_0-mlogloss:0.88732\n",
      "[79]\tvalidation_0-mlogloss:0.88393\n",
      "[80]\tvalidation_0-mlogloss:0.88063\n",
      "[81]\tvalidation_0-mlogloss:0.87740\n",
      "[82]\tvalidation_0-mlogloss:0.87424\n",
      "[83]\tvalidation_0-mlogloss:0.87116\n",
      "[84]\tvalidation_0-mlogloss:0.86821\n",
      "[85]\tvalidation_0-mlogloss:0.86533\n",
      "[86]\tvalidation_0-mlogloss:0.86250\n",
      "[87]\tvalidation_0-mlogloss:0.85970\n",
      "[88]\tvalidation_0-mlogloss:0.85695\n",
      "[89]\tvalidation_0-mlogloss:0.85433\n",
      "[90]\tvalidation_0-mlogloss:0.85177\n",
      "[91]\tvalidation_0-mlogloss:0.84927\n",
      "[92]\tvalidation_0-mlogloss:0.84681\n",
      "[93]\tvalidation_0-mlogloss:0.84445\n",
      "[94]\tvalidation_0-mlogloss:0.84218\n",
      "[95]\tvalidation_0-mlogloss:0.83991\n",
      "[96]\tvalidation_0-mlogloss:0.83758\n",
      "[97]\tvalidation_0-mlogloss:0.83538\n",
      "[98]\tvalidation_0-mlogloss:0.83306\n",
      "[99]\tvalidation_0-mlogloss:0.83098\n",
      "[100]\tvalidation_0-mlogloss:0.82890\n",
      "[101]\tvalidation_0-mlogloss:0.82681\n",
      "[102]\tvalidation_0-mlogloss:0.82475\n",
      "[103]\tvalidation_0-mlogloss:0.82282\n",
      "[104]\tvalidation_0-mlogloss:0.82097\n",
      "[105]\tvalidation_0-mlogloss:0.81917\n",
      "[106]\tvalidation_0-mlogloss:0.81736\n",
      "[107]\tvalidation_0-mlogloss:0.81555\n",
      "[108]\tvalidation_0-mlogloss:0.81377\n",
      "[109]\tvalidation_0-mlogloss:0.81211\n",
      "[110]\tvalidation_0-mlogloss:0.81051\n",
      "[111]\tvalidation_0-mlogloss:0.80887\n",
      "[112]\tvalidation_0-mlogloss:0.80738\n",
      "[113]\tvalidation_0-mlogloss:0.80597\n",
      "[114]\tvalidation_0-mlogloss:0.80453\n",
      "[115]\tvalidation_0-mlogloss:0.80320\n",
      "[116]\tvalidation_0-mlogloss:0.80179\n",
      "[117]\tvalidation_0-mlogloss:0.80043\n",
      "[118]\tvalidation_0-mlogloss:0.79904\n",
      "[119]\tvalidation_0-mlogloss:0.79769\n",
      "[120]\tvalidation_0-mlogloss:0.79641\n",
      "[121]\tvalidation_0-mlogloss:0.79514\n",
      "[122]\tvalidation_0-mlogloss:0.79390\n",
      "[123]\tvalidation_0-mlogloss:0.79264\n",
      "[124]\tvalidation_0-mlogloss:0.79144\n",
      "[125]\tvalidation_0-mlogloss:0.79028\n",
      "[126]\tvalidation_0-mlogloss:0.78915\n",
      "[127]\tvalidation_0-mlogloss:0.78803\n",
      "[128]\tvalidation_0-mlogloss:0.78692\n",
      "[129]\tvalidation_0-mlogloss:0.78581\n",
      "[130]\tvalidation_0-mlogloss:0.78475\n",
      "[131]\tvalidation_0-mlogloss:0.78377\n",
      "[132]\tvalidation_0-mlogloss:0.78281\n",
      "[133]\tvalidation_0-mlogloss:0.78172\n",
      "[134]\tvalidation_0-mlogloss:0.78078\n",
      "[135]\tvalidation_0-mlogloss:0.77985\n",
      "[136]\tvalidation_0-mlogloss:0.77890\n",
      "[137]\tvalidation_0-mlogloss:0.77806\n",
      "[138]\tvalidation_0-mlogloss:0.77724\n",
      "[139]\tvalidation_0-mlogloss:0.77634\n",
      "[140]\tvalidation_0-mlogloss:0.77544\n",
      "[141]\tvalidation_0-mlogloss:0.77455\n",
      "[142]\tvalidation_0-mlogloss:0.77366\n",
      "[143]\tvalidation_0-mlogloss:0.77292\n",
      "[144]\tvalidation_0-mlogloss:0.77207\n",
      "[145]\tvalidation_0-mlogloss:0.77127\n",
      "[146]\tvalidation_0-mlogloss:0.77055\n",
      "[147]\tvalidation_0-mlogloss:0.76980\n",
      "[148]\tvalidation_0-mlogloss:0.76907\n",
      "[149]\tvalidation_0-mlogloss:0.76833\n",
      "[150]\tvalidation_0-mlogloss:0.76760\n",
      "[151]\tvalidation_0-mlogloss:0.76684\n",
      "[152]\tvalidation_0-mlogloss:0.76619\n",
      "[153]\tvalidation_0-mlogloss:0.76553\n",
      "[154]\tvalidation_0-mlogloss:0.76484\n",
      "[155]\tvalidation_0-mlogloss:0.76422\n",
      "[156]\tvalidation_0-mlogloss:0.76357\n",
      "[157]\tvalidation_0-mlogloss:0.76298\n",
      "[158]\tvalidation_0-mlogloss:0.76233\n",
      "[159]\tvalidation_0-mlogloss:0.76173\n",
      "[160]\tvalidation_0-mlogloss:0.76121\n",
      "[161]\tvalidation_0-mlogloss:0.76070\n",
      "[162]\tvalidation_0-mlogloss:0.76024\n",
      "[163]\tvalidation_0-mlogloss:0.75968\n",
      "[164]\tvalidation_0-mlogloss:0.75915\n",
      "[165]\tvalidation_0-mlogloss:0.75873\n",
      "[166]\tvalidation_0-mlogloss:0.75823\n",
      "[167]\tvalidation_0-mlogloss:0.75778\n",
      "[168]\tvalidation_0-mlogloss:0.75735\n",
      "[169]\tvalidation_0-mlogloss:0.75688\n",
      "[170]\tvalidation_0-mlogloss:0.75641\n",
      "[171]\tvalidation_0-mlogloss:0.75592\n",
      "[172]\tvalidation_0-mlogloss:0.75553\n",
      "[173]\tvalidation_0-mlogloss:0.75515\n",
      "[174]\tvalidation_0-mlogloss:0.75475\n",
      "[175]\tvalidation_0-mlogloss:0.75433\n",
      "[176]\tvalidation_0-mlogloss:0.75394\n",
      "[177]\tvalidation_0-mlogloss:0.75360\n",
      "[178]\tvalidation_0-mlogloss:0.75323\n",
      "[179]\tvalidation_0-mlogloss:0.75283\n",
      "[180]\tvalidation_0-mlogloss:0.75237\n",
      "[181]\tvalidation_0-mlogloss:0.75204\n",
      "[182]\tvalidation_0-mlogloss:0.75166\n",
      "[183]\tvalidation_0-mlogloss:0.75127\n",
      "[184]\tvalidation_0-mlogloss:0.75086\n",
      "[185]\tvalidation_0-mlogloss:0.75040\n",
      "[186]\tvalidation_0-mlogloss:0.75005\n",
      "[187]\tvalidation_0-mlogloss:0.74972\n",
      "[188]\tvalidation_0-mlogloss:0.74942\n",
      "[189]\tvalidation_0-mlogloss:0.74912\n",
      "[190]\tvalidation_0-mlogloss:0.74875\n",
      "[191]\tvalidation_0-mlogloss:0.74845\n",
      "[192]\tvalidation_0-mlogloss:0.74816\n",
      "[193]\tvalidation_0-mlogloss:0.74783\n",
      "[194]\tvalidation_0-mlogloss:0.74750\n",
      "[195]\tvalidation_0-mlogloss:0.74725\n",
      "[196]\tvalidation_0-mlogloss:0.74705\n",
      "[197]\tvalidation_0-mlogloss:0.74677\n",
      "[198]\tvalidation_0-mlogloss:0.74646\n",
      "[199]\tvalidation_0-mlogloss:0.74629\n",
      "[200]\tvalidation_0-mlogloss:0.74604\n",
      "[201]\tvalidation_0-mlogloss:0.74581\n",
      "[202]\tvalidation_0-mlogloss:0.74555\n",
      "[203]\tvalidation_0-mlogloss:0.74530\n",
      "[204]\tvalidation_0-mlogloss:0.74512\n",
      "[205]\tvalidation_0-mlogloss:0.74487\n",
      "[206]\tvalidation_0-mlogloss:0.74460\n",
      "[207]\tvalidation_0-mlogloss:0.74442\n",
      "[208]\tvalidation_0-mlogloss:0.74414\n",
      "[209]\tvalidation_0-mlogloss:0.74396\n",
      "[210]\tvalidation_0-mlogloss:0.74373\n",
      "[211]\tvalidation_0-mlogloss:0.74353\n",
      "[212]\tvalidation_0-mlogloss:0.74330\n",
      "[213]\tvalidation_0-mlogloss:0.74302\n",
      "[214]\tvalidation_0-mlogloss:0.74282\n",
      "[215]\tvalidation_0-mlogloss:0.74259\n",
      "[216]\tvalidation_0-mlogloss:0.74244\n",
      "[217]\tvalidation_0-mlogloss:0.74231\n",
      "[218]\tvalidation_0-mlogloss:0.74214\n",
      "[219]\tvalidation_0-mlogloss:0.74193\n",
      "[220]\tvalidation_0-mlogloss:0.74168\n",
      "[221]\tvalidation_0-mlogloss:0.74154\n",
      "[222]\tvalidation_0-mlogloss:0.74138\n",
      "[223]\tvalidation_0-mlogloss:0.74118\n",
      "[224]\tvalidation_0-mlogloss:0.74093\n",
      "[225]\tvalidation_0-mlogloss:0.74077\n",
      "[226]\tvalidation_0-mlogloss:0.74068\n",
      "[227]\tvalidation_0-mlogloss:0.74046\n",
      "[228]\tvalidation_0-mlogloss:0.74032\n",
      "[229]\tvalidation_0-mlogloss:0.74015\n",
      "[230]\tvalidation_0-mlogloss:0.74006\n",
      "[231]\tvalidation_0-mlogloss:0.73994\n",
      "[232]\tvalidation_0-mlogloss:0.73975\n",
      "[233]\tvalidation_0-mlogloss:0.73959\n",
      "[234]\tvalidation_0-mlogloss:0.73940\n",
      "[235]\tvalidation_0-mlogloss:0.73923\n",
      "[236]\tvalidation_0-mlogloss:0.73909\n",
      "[237]\tvalidation_0-mlogloss:0.73887\n",
      "[238]\tvalidation_0-mlogloss:0.73874\n",
      "[239]\tvalidation_0-mlogloss:0.73854\n",
      "[240]\tvalidation_0-mlogloss:0.73843\n",
      "[241]\tvalidation_0-mlogloss:0.73834\n",
      "[242]\tvalidation_0-mlogloss:0.73818\n",
      "[243]\tvalidation_0-mlogloss:0.73813\n",
      "[244]\tvalidation_0-mlogloss:0.73802\n",
      "[245]\tvalidation_0-mlogloss:0.73787\n",
      "[246]\tvalidation_0-mlogloss:0.73775\n",
      "[247]\tvalidation_0-mlogloss:0.73761\n",
      "[248]\tvalidation_0-mlogloss:0.73750\n",
      "[249]\tvalidation_0-mlogloss:0.73740\n",
      "[250]\tvalidation_0-mlogloss:0.73721\n",
      "[251]\tvalidation_0-mlogloss:0.73712\n",
      "[252]\tvalidation_0-mlogloss:0.73697\n",
      "[253]\tvalidation_0-mlogloss:0.73690\n",
      "[254]\tvalidation_0-mlogloss:0.73680\n",
      "[255]\tvalidation_0-mlogloss:0.73668\n",
      "[256]\tvalidation_0-mlogloss:0.73658\n",
      "[257]\tvalidation_0-mlogloss:0.73656\n",
      "[258]\tvalidation_0-mlogloss:0.73642\n",
      "[259]\tvalidation_0-mlogloss:0.73633\n",
      "[260]\tvalidation_0-mlogloss:0.73627\n",
      "[261]\tvalidation_0-mlogloss:0.73614\n",
      "[262]\tvalidation_0-mlogloss:0.73604\n",
      "[263]\tvalidation_0-mlogloss:0.73593\n",
      "[264]\tvalidation_0-mlogloss:0.73590\n",
      "[265]\tvalidation_0-mlogloss:0.73578\n",
      "[266]\tvalidation_0-mlogloss:0.73571\n",
      "[267]\tvalidation_0-mlogloss:0.73566\n",
      "[268]\tvalidation_0-mlogloss:0.73554\n",
      "[269]\tvalidation_0-mlogloss:0.73547\n",
      "[270]\tvalidation_0-mlogloss:0.73537\n",
      "[271]\tvalidation_0-mlogloss:0.73527\n",
      "[272]\tvalidation_0-mlogloss:0.73517\n",
      "[273]\tvalidation_0-mlogloss:0.73513\n",
      "[274]\tvalidation_0-mlogloss:0.73506\n",
      "[275]\tvalidation_0-mlogloss:0.73497\n",
      "[276]\tvalidation_0-mlogloss:0.73490\n",
      "[277]\tvalidation_0-mlogloss:0.73485\n",
      "[278]\tvalidation_0-mlogloss:0.73475\n",
      "[279]\tvalidation_0-mlogloss:0.73469\n",
      "[280]\tvalidation_0-mlogloss:0.73472\n",
      "[281]\tvalidation_0-mlogloss:0.73468\n",
      "[282]\tvalidation_0-mlogloss:0.73462\n",
      "[283]\tvalidation_0-mlogloss:0.73457\n",
      "[284]\tvalidation_0-mlogloss:0.73449\n",
      "[285]\tvalidation_0-mlogloss:0.73439\n",
      "[286]\tvalidation_0-mlogloss:0.73435\n",
      "[287]\tvalidation_0-mlogloss:0.73433\n",
      "[288]\tvalidation_0-mlogloss:0.73421\n",
      "[289]\tvalidation_0-mlogloss:0.73420\n",
      "[290]\tvalidation_0-mlogloss:0.73414\n",
      "[291]\tvalidation_0-mlogloss:0.73409\n",
      "[292]\tvalidation_0-mlogloss:0.73404\n",
      "[293]\tvalidation_0-mlogloss:0.73404\n",
      "[294]\tvalidation_0-mlogloss:0.73405\n",
      "[295]\tvalidation_0-mlogloss:0.73400\n",
      "[296]\tvalidation_0-mlogloss:0.73405\n",
      "[297]\tvalidation_0-mlogloss:0.73397\n",
      "[298]\tvalidation_0-mlogloss:0.73393\n",
      "[299]\tvalidation_0-mlogloss:0.73386\n",
      "[300]\tvalidation_0-mlogloss:0.73381\n",
      "[301]\tvalidation_0-mlogloss:0.73376\n",
      "[302]\tvalidation_0-mlogloss:0.73378\n",
      "[303]\tvalidation_0-mlogloss:0.73371\n",
      "[304]\tvalidation_0-mlogloss:0.73376\n",
      "[305]\tvalidation_0-mlogloss:0.73372\n",
      "[306]\tvalidation_0-mlogloss:0.73373\n",
      "[307]\tvalidation_0-mlogloss:0.73371\n",
      "[308]\tvalidation_0-mlogloss:0.73371\n",
      "[309]\tvalidation_0-mlogloss:0.73365\n",
      "[310]\tvalidation_0-mlogloss:0.73365\n",
      "[311]\tvalidation_0-mlogloss:0.73365\n",
      "[312]\tvalidation_0-mlogloss:0.73365\n",
      "[313]\tvalidation_0-mlogloss:0.73368\n",
      "[314]\tvalidation_0-mlogloss:0.73365\n",
      "[315]\tvalidation_0-mlogloss:0.73360\n",
      "[316]\tvalidation_0-mlogloss:0.73359\n",
      "[317]\tvalidation_0-mlogloss:0.73366\n",
      "[318]\tvalidation_0-mlogloss:0.73361\n",
      "[319]\tvalidation_0-mlogloss:0.73363\n",
      "[320]\tvalidation_0-mlogloss:0.73359\n",
      "[321]\tvalidation_0-mlogloss:0.73357\n",
      "[322]\tvalidation_0-mlogloss:0.73357\n",
      "[323]\tvalidation_0-mlogloss:0.73359\n",
      "[324]\tvalidation_0-mlogloss:0.73362\n",
      "[325]\tvalidation_0-mlogloss:0.73356\n",
      "[326]\tvalidation_0-mlogloss:0.73349\n",
      "[327]\tvalidation_0-mlogloss:0.73347\n",
      "[328]\tvalidation_0-mlogloss:0.73343\n",
      "[329]\tvalidation_0-mlogloss:0.73347\n",
      "[330]\tvalidation_0-mlogloss:0.73347\n",
      "[331]\tvalidation_0-mlogloss:0.73345\n",
      "[332]\tvalidation_0-mlogloss:0.73348\n",
      "[333]\tvalidation_0-mlogloss:0.73351\n",
      "[334]\tvalidation_0-mlogloss:0.73354\n",
      "[335]\tvalidation_0-mlogloss:0.73356\n",
      "[336]\tvalidation_0-mlogloss:0.73354\n",
      "[337]\tvalidation_0-mlogloss:0.73362\n",
      "[338]\tvalidation_0-mlogloss:0.73369\n",
      "[339]\tvalidation_0-mlogloss:0.73372\n",
      "[340]\tvalidation_0-mlogloss:0.73371\n",
      "[341]\tvalidation_0-mlogloss:0.73368\n",
      "[342]\tvalidation_0-mlogloss:0.73378\n",
      "[343]\tvalidation_0-mlogloss:0.73375\n",
      "[344]\tvalidation_0-mlogloss:0.73381\n",
      "[345]\tvalidation_0-mlogloss:0.73385\n",
      "[346]\tvalidation_0-mlogloss:0.73386\n",
      "[347]\tvalidation_0-mlogloss:0.73392\n",
      "[348]\tvalidation_0-mlogloss:0.73390\n",
      "[349]\tvalidation_0-mlogloss:0.73391\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=0, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=350, n_jobs=-1, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=0, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=350, n_jobs=-1, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=0, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=350, n_jobs=-1, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(n_estimators=350, max_depth=10, learning_rate=0.03, n_jobs=-1, gpu_id=0, tree_method=\"gpu_hist\")\n",
    "clf.fit(x_train, y_train, eval_set=[(x_val, y_val)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348760a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80e692ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8945240101095198, 0.8631036584015404)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds = clf.predict(X_train_all)\n",
    "accuracy_score(y_train_all, train_preds), balanced_accuracy_score(y_train_all, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ce64a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6566421098664513, 0.4884903192200657)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "accuracy_score(y_test, preds), balanced_accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89a08992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 238,  519,   68,   38,   26,    5,    0,    0],\n",
       "       [ 165, 6773,  974,  212,  193,   40,    5,    0],\n",
       "       [  17,  550, 6005, 1033,  886,  538,   51,   10],\n",
       "       [  10,  156, 1010, 2387,  360,  243,   28,    2],\n",
       "       [   5,  120,  895,  236, 2435,  366,   16,    0],\n",
       "       [   4,   40,  310,   58,  170, 1483,   58,   12],\n",
       "       [   0,   12,  140,   43,   66,  209,  191,   13],\n",
       "       [   2,    8,  105,   33,   22,   98,   27,    8]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb4a50",
   "metadata": {},
   "source": [
    "Overall, clearly the model is overfitting and is unable to adapt to the test data. Nevertheless, this is a good start.\n",
    "\n",
    "- We can add features related to work experience descriptions to see if it improves the performance.\n",
    "- Another possible area of improvement is looking at the skills/titles/degrees and how they differ between train and test data. Since we are using CountVectorizer, it is quite possible that the top n-grams in train and test are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae8f5a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
